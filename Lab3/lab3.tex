
% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and republish this document.

\documentclass{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multicol, caption}
\usepackage{amsmath}

\newenvironment{Figure}
	{\par\medskip\noindent\minipage{\linewidth}}
	{\endminipage\par\medskip}

\setlength{\columnsep}{2cm}
\usepackage[a4paper, total={7in, 8.5in}]{geometry}

\setlength{\topmargin}{-0.6in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{9.7in}

% Define new length
\newlength{\figureWidth}
% Set the new length to a specific value
\setlength{\figureWidth}{3in}

\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\lstset{language=Matlab,%
	%basicstyle=\color{red},
	basicstyle=\footnotesize\texttt{},
	breaklines=true,%
	morekeywords={matlab2tikz},
	keywordstyle=\color{blue},%
	morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
	identifierstyle=\color{black},%
	stringstyle=\color{mylilas},
	commentstyle=\color{mygreen},%
	showstringspaces=false,%without this there will be a symbol in the places where there is a space
	numbers=left,%
	numberstyle={\tiny \color{black}},% size of the numbers
	numbersep=9pt, % this defines how far the numbers are from the text
	emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
	%emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

\definecolor{lightgray}{gray}{0.5}
\setlength{\parindent}{0pt}


\begin{document}
	
	\title{Machine Learning Lab Three COMP3206}
	\author{Alois Klink \and \href{mailto:ak9g14@soton.ac.uk}{ak9g14@soton.ac.uk} \and 2704 7555}
	\maketitle
	
\begin{multicols}{2}[
	\begin{par}
		This Lab covers projecting data, creating ROC Curves, Nearest-Neighbour and Distance-to-Mean Classifiers and the generic Baye's Optimal Class Boundary.
		\url{https://secure.ecs.soton.ac.uk/notes/comp3206/ml_lab_three2016.pdf}
	\end{par}
	]

\section{Create Data and Plot Contours}

\begin{par}
	Creates $ X \in \omega_i \sim \mathcal{N}_2 (\mathbf{m}_i, \mathbf{C}_i)$ where $ \mathbf{m}_1 = \begin{bmatrix}
	0 & 2
	\end{bmatrix}^{t} $, $ \mathbf{m_2} = \begin{bmatrix}
	1.7 & 2.5
	\end{bmatrix}^{t} $, and $ \mathbf{C_1} = \mathbf{C_2} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} $. The plot can be seen at Figure \ref{fig:dataPlot}.
\end{par} \vspace{0.2em}
\begin{lstlisting}
m1 = [0 2]';
m2 = [1.7 2.5]';
sigma = [2 1; 1 2];
C1 = sigma;
C2 = sigma;
numGrid = 50;

xRange = linspace(-6.0, 6.0, numGrid);
yRange = linspace(-6.0, 6.0, numGrid);

P1 = zeros(numGrid, numGrid);
P2 = P1;
for i=1:numGrid
for j=1:numGrid;
x = [yRange(j) xRange(i)]';
P1(i,j) = mvnpdf(x', m1', C1);
P2(i,j) = mvnpdf(x', m2', C2);
end
end

Pmax = max(max([P1 P2]));

figure(1), clf,
% drawing contour lines at P = 10%, 50%, 80%
contour(xRange.', yRange, P1, [0.1*Pmax 0.5*Pmax 0.8*Pmax], 'LineWidth', 2);
hold on
plot(m1(1), m1(2), 'b*', 'LineWidth', 4);
contour(xRange.', yRange, P2, [0.1*Pmax 0.5*Pmax 0.8*Pmax], 'LineWidth', 2);
plot(m2(1), m2(2), 'r*', 'LineWidth', 4);
\end{lstlisting}

\section{Draw Data}

\begin{lstlisting}
N = 200;
X1 = mvnrnd(m1, C1, N);
X2 = mvnrnd(m2, C2, N);
plot(X1(:,1),X1(:,2),'bx', X2(:,1),X2(:,2),'ro');grid on
\end{lstlisting}

\section{Fischer Linear Discriminant}

\begin{par}
	The Fischer Linear Disciminant is the direction on which projected data is maximally separable, which can be calculated from:
\end{par}
	$$\mathbf{w}_F = \alpha \mathbf{C}_W ^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$$
\begin{lstlisting}
wF = (C1+C2)\(m1-m2);
xx = -6:0.1:6;
yy = xx*wF(2)/wF(1);
plot(xx,yy,'r', 'LineWidth', 2);
\end{lstlisting}

\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_03.eps}
	\captionof{figure}{Data Plot, Contours, and Fischer Discriminant}
	\label{fig:dataPlot}
\end{Figure}


\section{Histograms of Projected Data}

\begin{par}
	Projects the data onto the Fischer Direction and plots a histogram.
\end{par} \vspace{0.2em}
\begin{lstlisting}
p1 = X1*wF;
p2 = X2*wF;
plo = min([p1; p2]);
phi = max([p1; p2]);
[nn1, xx1] = hist(p1);
[nn2, xx2] = hist(p2);
hhi = max([nn1 nn2]);
figure(2),
subplot(211), bar(xx1, nn1);
axis([plo phi 0 hhi]);
title('Distribution of Projections', 'FontSize', 16)
ylabel('Class 1', 'FontSize', 14)
subplot(212), bar(xx2, nn2);
axis([plo phi 0 hhi])
ylabel('Class 2', 'FontSize', 14)
\end{lstlisting}

\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_04.eps}
	\captionof{figure}{Fischer Projection Histograms}
\end{Figure}


\section{ROC Curve}

\begin{par}
	Creates a Reciceiver Operating Characteristic (ROC) Curve that plots True Positive (TP) and False Positive (FP) rates.
\end{par} \vspace{0.2em}
\begin{lstlisting}
[ROC, areaROC] = calculateROC( p1, p2, 50);
figure(3), clf, plot(ROC(:,1), ROC(:,2), 'b', 'LineWidth', 2);
axis([0 100 0 100]); grid on, hold on; plot(0:100, 0:100, 'b-');
xlabel('False Positive'); ylabel('True Positive');
title('Receiver Operating Characteristic Curve');
\end{lstlisting}

\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_05.eps}
	\captionof{figure}{Receiver Operating Characteristic Curve}
\end{Figure}


\section{ROC Curve Area}

\begin{lstlisting}
disp(['The area of the ROC is ' num2str(areaROC)]);
\end{lstlisting}

\color{lightgray} \begin{verbatim}The area of the ROC is 7424.625
\end{verbatim} \color{black}


\section{Classification Accuracy}

\begin{par}
	Computes the classification accuracy for the True Positive rate of $ 70\% $.
\end{par} \vspace{0.2em}
\begin{lstlisting}
truePositiveToFind = 70;
[minValue, closestIndex] = min( abs( ROC(:,2) - truePositiveToFind) );
threshold = ROC(closestIndex, 3);
truePositives = length(find(p1 > threshold))*100 / N
falsePositives = length(find(p2 > threshold))*100 / N
\end{lstlisting} \begin{verbatim}truePositives = 72
falsePositives = 20
\end{verbatim} \color{black}

\section{More ROC Curves}

\begin{par}
	Figure \ref{fig:moreROC} shows the ROC Curves for the data projected onto a random direction and onto the direction connecting the two means.
\end{par} \vspace{0.2em}
\begin{lstlisting}
randAng = rand() * 2* pi; %random direction
randomW = [sin(randAng) cos(randAng)]'; %random direction
[randROC, randROCArea] = calculateROC( X1*randomW, X2*randomW, 50);
disp(['The area of the RandROC is ' num2str(randROCArea)]);

betweenMeanW = m1 - m2;
[betMROC, betMROCArea] = calculateROC( X1*betweenMeanW, X2*betweenMeanW, 50);
disp(['The area of the betweenMeansROC is ' num2str(betMROCArea)]);

figure(4), clf, plot(randROC(:,1), randROC(:,2), 'b', 'LineWidth', 2); hold on;
plot(betMROC(:,1),betMROC(:,2)); legend('Random Direction ROC', 'Between Mean ROC');
axis([0 100 0 100]); grid on; plot(0:100, 0:100, 'b-');
xlabel('False Positive'); ylabel('True Positive');
title('Receiver Operating Characteristic Curve'); hold off;
\end{lstlisting}

\color{lightgray} \begin{verbatim}
The area of the RandROC is 7101
The area of the betweenMeansROC is 7110.375
\end{verbatim} \color{black}

\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_06.eps}
	\captionof{figure}{More ROC Curves}
	\label{fig:moreROC}
\end{Figure}


\section{Nearest-Neighbour Classifier}

\begin{par}
	The Nearest-Neighbour Classifier finds the closest training point to the test point, and uses its class to classify the test data.
\end{par} \vspace{0.2em}
\begin{lstlisting}
X = [X1; X2];
N1 = size(X1, 1);
N2 = size(X2, 1);
y = [ones(N1,1); -1*ones(N2,1)];
d = zeros(N1+N2-1,1);
nCorrect = 0;
for jtst = 1:(N1+N2)
% pick a point to test
xtst = X(jtst,:);
ytst = y(jtst);
% All others form the training set
jtr = setdiff(1:N1+N2, jtst);
Xtr = X(jtr,:);
ytr = y(jtr,1);
% Compute all distances from test to training points
for i=1:(N1+N2-1)
d(i) = norm(Xtr(i,:)-xtst);
end
% Which one is the closest?
[imin] = find(d == min(d));
% Does the nearest point have the same class label?
if ( ytr(imin(1)) * ytst > 0 )
nCorrect = nCorrect + 1;
end
end

% Percentage correct
pCorrect = nCorrect*100/(N1+N2);
disp(['Nearest neighbour accuracy: ' num2str(pCorrect) '%']);
\end{lstlisting}

\color{lightgray} \begin{verbatim}Nearest neighbour accuracy: 57.75%
\end{verbatim} \color{black}


\section{Distance-to-Mean Classifiers}

\begin{par}
	The following code shows two Distance-to-Mean Classifiers, which classify test data depending on the distance between it and the means of the training data. The Euclidean Distance classifier uses the normal $ distance = \sqrt{\sum_{i=1}^n (q_i-p_i)^2} $ formula. However, the \href{https://en.wikipedia.org/wiki/Mahalanobis_distance}{Mahalanobis distance} takes into account variance and correlation, thus being a bit measure of distance.
\end{par} \vspace{0.2em}
\begin{lstlisting}
for jtst = 1:(N1+N2)
% pick a point to test
xtst = X(jtst,:);
ytst = y(jtst);
% All others form the training set
jtr = setdiff(1:N1+N2, jtst);
Xtr = X(jtr,:);
ytr = y(jtr,1);
% find means and euclidian distances
mahDistance1 = mean(Xtr((ytr > 0),:)) - xtst;
mahDistance2 = mean(Xtr((ytr < 0),:)) - xtst;
% compare distances
eucDistance = @(x) sqrt(x(1)^2 + x(2)^2);
if eucDistance(mahDistance1) < eucDistance(mahDistance2)
yVal = 1;
else
yVal = -1;
end
% Does the nearest point have the same class label?
if yVal * ytst > 0
eucCorrect = eucCorrect + 1;
end

% find Mahalanobis distances
mahDistance1 = mahal(xtst, Xtr(ytr > 0,:));
mahDistance2 = mahal(xtst, Xtr(ytr < 0,:));
% compare distances
if mahDistance1 < mahDistance2
yVal = 1;
else
yVal = -1;
end
% Does the nearest point have the same class label?
if yVal * ytst > 0
mahCorrect = mahCorrect + 1;
end
end
% Percentage correct
eucPCorrect = eucCorrect*100/(N1+N2);
disp(['Euclidiean Distance-to-mean accuracy: ' num2str(eucPCorrect) '%']);
mahPCorrect = mahCorrect*100/(N1+N2);
disp(['Mahalanobis Distance-to-mean accuracy: ' num2str(mahPCorrect) '%']);
\end{lstlisting}

\color{lightgray} \begin{verbatim}Euclidiean Distance-to-mean accuracy: 72%
Mahalanobis Distance-to-mean accuracy: 76.25%
\end{verbatim} \color{black}


\section{Posterior Probability}

\begin{par}
	Figure \ref{fig:postProb} shows $ P (\omega_1 | \mathbf{x}) $, i.e. the probability that point $ \mathbf{x} $ belongs to class $ \omega_1 $.
\end{par} \vspace{0.2em}
\begin{lstlisting}
classDistribution = gmdistribution([m1'; m2'], cat(3, C1, C2));
clear X; clear y;
X(:,1) = -5:0.1:5; Y = X(:,1);
y(1:size(X,1),1) = Y(1);
postProb = posterior(classDistribution, [ X(:,1) y(:,1)]);
postProb2 = postProb(:,2);
for i = 2:size(Y,1)
clear y; y(1:size(X,1),1) = Y(i);
postProb = posterior(classDistribution, [ X(:,1) y(:,1)]);
postProb2 = [postProb2 postProb(:,2)];
end
surf(X, Y, postProb2); xlabel('X_1'); ylabel('X_2'); zlabel('$P (\omega_1 | \mathbf{x})$');
title('Posterior Probability');
\end{lstlisting}

\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_07.eps}
	\captionof{figure}{Posterior Probability}
	\label{fig:postProb}
\end{Figure}


\section{Baye's Optimal Class Boundary}

\begin{par}
	Calculates and shows in Figure \ref{fig:bayesOpt} the Baye's Optimal Class Boundary if both the means $ \mathbf{m}_i $ and the covariances $ \mathbf{C}_i $ are different. 
	As the Baye's Classifier Decision Rule is: 
	$$ f(\mathbf{x}) = \begin{cases} \omega_1, & (P (\omega_1 | \mathbf{x}) > P (\omega_2 | \mathbf{x}) \\ \omega_2, & (P (\omega_1 | \mathbf{x}) < P (\omega_2 | \mathbf{x}) \\ \end{cases} $$ and the Posterior Probability can be described by \href{https://en.wikipedia.org/wiki/Bayes%27_theorem}{Baye's Theorem}
	$$ P (\omega_i | \mathbf{x}) = \frac{1}{c} P (\mathbf{x} | \omega_i) P(\omega_i) $$ 
	and the Normal Distribution probability is given by: $$ P (\mathbf{x} | \omega_i) = \mathcal{N}(\boldsymbol m_i, \boldsymbol C_i) $$
	$$ = \frac{1}{\sqrt{(2\pi)^2|C|}} \exp(-\frac{1}{2} (\mathbf{x} - \mathbf{m}_i)^T \mathbf{C}_i^{-1} (\mathbf{x} - \mathbf{m}_i) ) $$
	the Baye's Optimal Class Boundary can be calculated using the following code.
\end{par} \vspace{0.2em}
\begin{lstlisting}
% I know the Lab instrunctions said to write out the algebra but the University pays good money for MATLAB and I am lazy
C2 = [1.5 0; 0 1.5];
X2 = mvnrnd(m2, C2, N);
plot(X1(:,1),X1(:,2),'bx', X2(:,1),X2(:,2),'ro');grid on;hold on;

normalProb = @(x, mean, covar) ((1/( ((2*pi)^(size(mean,1)/2))*det(covar)^(1/2))) * exp(-1/2*(x'-mean)'/(covar)*(x'-mean)));
varX = sym('x', [1 2], 'real');
eqn = normalProb(varX, m1, C1)*.5 == normalProb(varX, m2, C2)*.5;
[solx, ~, ~, ~] = solve(eqn,varX, 'ReturnConditions', true);
clear x
x(1,:) = -10:0.123:10; x(2,:) = subs(solx, x(1,:)); axis([-6 6 -6 6]);
plot(x(1,:), x(2,:), 'DisplayName', 'Bayes Optimal Class Boundary');
\end{lstlisting}
	
\begin{Figure}
	\centering
	\includegraphics [width=\figureWidth]{html/labthree_08.eps}
	\captionof{figure}{Baye's Optimal Class Boundary}
	\label{fig:bayesOpt}
\end{Figure}

\end{multicols}

\end{document}
    
exit