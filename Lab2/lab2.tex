
% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and republish this document.

\documentclass{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multicol, caption}
\usepackage{amsmath}
\usepackage{textcomp}

\newenvironment{Figure}
	{\par\medskip\noindent\minipage{\linewidth}}
	{\endminipage\par\medskip}

\setlength{\columnsep}{2cm}
\usepackage[a4paper, total={7in, 8.5in}]{geometry}

\setlength{\topmargin}{-0.6in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{9.7in}

% Define new length
\newlength{\figureWidth}
% Set the new length to a specific value
\setlength{\figureWidth}{3in}

\newcommand{\makeFigure}[3]{
	\begin{Figure}
		\centering
		\includegraphics [width=\figureWidth]{html/{#1}}
		\captionof{figure}{#2}
		\label{fig:#3}
	\end{Figure}
}

\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\lstset{language=Matlab,%
	%basicstyle=\color{red},
	basicstyle=\footnotesize\texttt{},
	breaklines=true,%
	upquote=true,%
	morekeywords={matlab2tikz},
	keywordstyle=\color{blue},%
	morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
	identifierstyle=\color{black},%
	stringstyle=\color{mylilas},
	commentstyle=\color{mygreen},%
	showstringspaces=false,%without this there will be a symbol in the places where there is a space
	numbers=left,%
	numberstyle={\tiny \color{black}},% size of the numbers
	numbersep=9pt, % this defines how far the numbers are from the text
	emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
	%emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

\definecolor{lightgray}{gray}{0.5}
\setlength{\parindent}{0pt}


\begin{document}
	
	\title{Machine Learning Lab Two COMP3206}
	\author{Alois Klink \and \href{mailto:ak9g14@soton.ac.uk}{ak9g14@soton.ac.uk} \and 2704 7555}
	\maketitle
	
\begin{multicols}{2}[
	\begin{par}
		This Lab covers generating a Bayes Optimal Decision Boundary and a Percepton's Learning Algorithm's Decision Boundary,
		\url{https://secure.ecs.soton.ac.uk/notes/comp6229/ml_lab_two2016.pdf}
	\end{par}
	]
	
\tableofcontents

\section{Finding the Perpendicular Distance to Origin}

\begin{par}
The following code creates a line, finds the angle of the line, then the perpendicular angle, and finally use the perpendicular angle and the distance given by the equation \(-c/\sqrt{a^2_1+b^2_1}\) to create a line from the origin, which should meet the original line perfectly.
\end{par}
\begin{lstlisting}
plot([0 -4], [-3 0], 'b', 'LineWidth', 2, 'DisplayName', 'Original Line');
title('Finding the Perpendicular Distance to Origin');
axis([-5 1 -5 1]); grid on;
hold on;
perp(1, :) = -5:0.1:5;
a = 3;
b = 4;
c = 12;
gradient = a/-b;

%distance equation given to us
distance = -c/sqrt(a^2 + b^2);

%calculating the angle with trigonometry
angle(1) = atan(gradient(1));
%calculating the perpendicular angle by adding pi/2
angle(2) = angle(1) + pi/2;

% draws a line with the perpendicular angle and distance given
plot([0 distance*cos(angle(2))], [0 distance*sin(angle(2))], 'DisplayName','Perpendicular Distance to Origin');
legend('show'); hold off;
\end{lstlisting}
\makeFigure{labtwo_01.eps}{Perpendicular Distance to Origin}{perpDist}
\begin{par}
As can be seen in Figure \ref{fig:perpDist}, the above line is perpendicular with the the original line, goes to the origin, is the same length as the distance given, and meets the line.
\end{par}


\section{Generating Samples from Bi-Variate Normal Densities with Distinct Means}

\begin{par}
	The plots of the bi-variate normally distributed denseties with distinct means can be found in Figure \ref{fig:boundaries}.
\end{par}

\begin{lstlisting}
means(1,:) = [0   2].';
means(2,:) = [1.5 0].';
sigma = [2 1; 1 2];
valuesSize = 100;

X1 = mvnrnd(means(1, :).', sigma, valuesSize);
X2 = mvnrnd(means(2, :).', sigma, valuesSize);

plot(X1(:,1),X1(:,2),'r.', 'DisplayName', 'Class +1');
hold on; title('Data and Class Boundaries');
plot(X2(:,1),X2(:,2),'mx', 'DisplayName', 'Class -1');
axis([-5 5 -5 5]);
\end{lstlisting}

\section{Computing Bayes Optimal Class Boundry}

\begin{par}
	Solves the solutions for the equation \(\boldsymbol{w}^{T} \times \boldsymbol{x} + b = 0 \) using MATLAB's Symbolic Math Toolbox. The solutions form the Bayes Optimal Class Boundry, and it is located where \(P(\omega_1 | \boldsymbol{x}) = P(\omega_2 | \boldsymbol{x})\), ie it is difficult to classify a point into either classes.
	This line is drawn in Figure \ref{fig:boundaries}.
\end{par}
\begin{lstlisting}
invSig = inv(sigma);
w = 2*invSig*(means(1,:).' - means(2,:).');
b = ( transpose(means(1,:).')*invSig*means(1,:).' - ...
    ( transpose(means(2,:).')*invSig*means(2,:).')) - ...
    log(1);
varX = sym('x', [2 1]);
eqn = transpose(w)*varX + b == 0;
[solx, ~, ~, ~] = solve(eqn,varX, 'ReturnConditions', true);
x(1,:) = [-5 5]; x(2,:) = subs(solx, x(1,:));
plot(x(1,:), x(2,:), 'DisplayName', 'Bayes Optimal Class Boundary');
\end{lstlisting}

\section{Perceptron Learning Algorithm}
\begin{par}
	The Perceptron Learning Algorithm is a linear classifier where the output is:
	\[f(x) = 
		\begin{cases}
		1 & \text{if } \boldsymbol{x} \cdot \boldsymbol{w} > 0\\
		-1 & \text{if } \boldsymbol{x} \cdot \boldsymbol{w} < 0\\
		\end{cases}
	\]
	If it classifies the output incorrectly, \(\boldsymbol{w}\), the weight vector, is modified so that it goes closer to the incorrect point.
\end{par}

\begin{lstlisting}
data = [X1 ones(valuesSize,1) -1*ones(valuesSize,1)];
data = [data; X2 ones(valuesSize,1) 1*ones(valuesSize,1)];
valuesSize = size(data);
selection = randperm(valuesSize(1));
trainingPercentage = 0.8;

% seperating data into training and test data
trainingData = data(selection(1:valuesSize(1)*trainingPercentage),:);
testData = data(selection(valuesSize(1)*trainingPercentage+1:valuesSize(1)),:);

weights = zeros(1,3);
eta = 0.001;
trainingDataSize = size(trainingData);
for iter=1:10000
	j = randi([1 trainingDataSize(1)]);
	result = dot(trainingData(j,1:3),weights);
	if result >= 0
		result = 1;
	else
		result = -1;
	end
	% divided by 2 because we want error to be 1, 0, or -1.
	error = (trainingData(j,4) - result)/2;
	weights = weights + eta * error * trainingData(j,1:3);
end
testDataSize = size(testData);
success = 0;
for iter=1:testDataSize(1)
	result = dot(testData(iter,1:3),weights);
	if result * testData(iter, 4) > 0
		success = success + 1;
	end
end
SuccessRatio = success/testDataSize(1);
disp(['Success Ratio =' num2str(SuccessRatio)]);

input = sym('X', [2 1]); varIn = [input; 1];
eqn = dot(varIn,weights) == 0;
[solx, ~, ~, ~] = solve(eqn,input, 'ReturnConditions', true);
x(1,:) = [-5 5]; x(2,:) = subs(solx, x(1,:));
plot(x(1,:), x(2,:), 'r', 'DisplayName', 'Perceptron Boundary');
legend('show'); hold off;
\end{lstlisting}

        \color{lightgray} \begin{verbatim}Success Ratio =0.9
\end{verbatim} \color{black}

\makeFigure{labtwo_04.eps}{Perpendicular Distance to Origin}{boundaries}

\begin{par}
As you can see in Figure \ref{fig:boundaries}, the boundary of the Perceptron Learning Algorithm is quite similar to the Bayes Optimal Class Boundry, but not perfect. As \(\eta\) (eta) and the max number of iterations are increased, the accuracy of the Perceptron Learning Algorithm increases, but it reaches a limit as it is impossible to classifly all the data correctly, due to the nature of a Normal Distribution having a range of infinity.
\end{par}

\end{multicols}

\end{document}
    
exit
